#YOLO v2

#翻译：http://noahsnail.com/2017/12/26/2017-12-26-YOLO9000,%20Better,%20Faster,%20Stronger%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/

### 概要

目前分类数据集庞大，但是相比之下用于识别的数据集相差很多，于是提出了一种新方法来利用我们已经拥有的大量分类数据，并使用它来扩大当前检测系统的范围。方法使用目标分类的层次视图，允许我们将不同的数据集合在一起。
还提出了联合训练算法，允许在检测和分类数据上训练目标检测器。方法利用标记的检测图像来学习精确地定位目标，同时使用分类图像来增加其词汇和鲁棒性。
指出yolo v1的缺点：大量定位误差和低召回率，于是针对这两点进行改进的同时保持分类精度

一个区别于常见的思维：计算机视觉一般趋向于更大，更深的网络[6][18][17]。更好的性能通常取决于训练更大的网络或将多个模型组合在一起。
但是，在YOLOv2中，我们需要一个更精确的检测器，它仍然很快。不是扩大我们的网络，而是简化网络，然后让表示更容易学习。将过去的工作与新概念汇集起来，以提高YOLO的性能


### 架构
相对v1做了技术对比

更好：
1.BN 收敛性的显著改善，同时消除了对其他形式正则化的需求，有助于模型的正则化。
2.高分辨率分类器，v2以448×448的分辨率对分类网络进行10个迭代周期的微调,v1用224x224训练
3.具有锚盒的卷积，类似faster rcnn的rpn，使用anchor来做定位回归，加入anchor后虽然mAP小幅下降，但是在召回率上得到了较大的提升
4.维度聚类，这是有别于之前anchor机制的一个升级。之前的anchor尺寸都是人为设定的，而v2中用聚类去数据集上做先验，更到更好的尺寸
5.直接位置预测，不预测其偏移量
6.细粒度功能，没有像SSD一样借鉴FDN的方式，只是仅仅添加一个通道层，从26x26分辨率的更早层中提取特征。
7.多尺度训练，只使用卷积层和池化层，每隔10个批次我们的网络会随机选择一个新的图像尺寸大小。

更快：
8.改进backbone网络，相当于yolo v1采用的googleNet的一个进化，Darknet-19，基本设计理念都是借鉴了googleNet。

更强：
9.联合训练分类和检测数据的机制，标记为检测的图像时，基于完整的YOLOv2损失函数进行反向传播。一个分类图像时，只从该架构的分类特定部分反向传播损失。
由于检测数据集只有通用目标和通用标签，而ImageNet有更细的划分，于是引入WordTree,可以在共同的下义词上执行多次softmax操作.
检测器预测边界框和概率树。我们遍历树，在每个分割中采用最高的置信度路径，直到达到某个阈值，然后我们预测目标类。

### 训练


### 思考
相比v1，更好，更快，更强
主要技术革新对应（增加anchor机制，优化的网络骨架，联合训练）

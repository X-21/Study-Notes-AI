# LeNet
#翻译：https://blog.csdn.net/qianqing13579/article/details/71076261

### 概要

LeNet正式提出cnn的思想，则AlexNet奠定了cnn的地位。
将图像分类从传统的分类方式正式带向cnn方向，由于数据和GPU的计算力共同提高下，构建了当时最大的卷积神经网络，并获得了最优结果。
同时论文突出了两方面的问题：
1.怎样在这个大型网络下提高性能的同时减少训练时间。
2.大型网络有大量的参数，怎样去防止过拟合。


### 怎样在这个大型网络下提高性能的同时减少训练时间

+ 提出ReLU激活函数
在CIFAR-10数据集上达到25%训练错误率比同等条件下使用tanh神经元快6倍
+ 多GPU训练
使用多GPU训练一方面是受单个GPU的显存限制，一方面是加快训练
在多GPU之间并不是所有层都互相通信，只在某些层进行交互通信。
+ 局部响应归一化
在论文中局部归一化确实使性能有所提高
（响应归一化将top-1和top-5的错误率分别降低了1.4%和1.2%）
但是在后续研究中，对于越大的网络。局部响应归一化并没有带来提升，所以这个技术被淡化，基本不再使用。
+ 重叠池化
池化步长小于池化尺寸就是重叠池化
实验证明重叠池化带来了性能上的提升（将top-1和top-5的错误率分别降低了0.4%和0.3%）
重叠池化也在一定程度上抑制了过拟合

【我的思考】
1.relu激活函数的特性让求导变得更加简单
2.多GPU让所有层都进行交互会产生怎样的区别？

###大型网络有大量的参数，怎样去防止过拟合

+ 数据增强
降低过拟合最简单直接的方式就是扩大数据集
论文中扩大数据集的方式：
1.从256x256的图像中随机提取224x224大小的块以及他们的翻转图像构成新的数据集（可扩展(256-224)2*2=2048倍）
预测方针是：提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。
2.改变训练图像的RGB通道的强度
相当于改变了图像的光照、颜色、强度，但是目标特性任然不变。

+ dropout
每一个隐藏神经元都有0.5的概率不参与前向传播和反向传播。
每一次迭代隐藏神经元的训练对象都是在变的

【我的思考】
1.越大的网络对数据的依赖越大，训练复杂网络必须要有足够的数据量支撑，在相似分类的情况下可以使用迁移学习从一定程度上减小这种依赖
2.drop让更多的隐藏神经元参与进来，让激活较少的神经元得到迭代更新，使得网络泛华能力更强



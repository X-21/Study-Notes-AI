# LeNet
翻译：https://blog.csdn.net/qianqing13579/article/details/71076261
参考：https://www.jianshu.com/p/9997c6f5c01e
https://cuijiahua.com/blog/2018/01/dl_3.html
https://blog.csdn.net/qianqing13579/article/details/71076261
https://blog.csdn.net/zoeyunjj/article/details/51643650
##内容简述
第一部分：简述。 
不管是语音、图形、还是其他模式类型，模式识别纯靠手动设计是不可能的。过去使用两个模块来进行，第一个模块是特征提取器，第二个模块是可训练的分类器，分类器的适用范围又限制在了低维度空间的容易分别的类别，因此又需要合适的特征提取器去配合工作。但是过去十年中又有三方面的因素结合改变了这一现象：1.成本低、运算速度快的机器慢慢普及，使得可以直接使用“暴力计算”而不用太大程度依赖于计算方法的优化；2.大型数据集的可用性使得设计人员在设计识别系统时可以更多地依赖数据本身而不是依赖手动设计的特征提取器；3.最重要的一点是，在大数据集下，强大的机器学习技术变得可用了，也使得可以处理高维度输入、生成更复杂的决策函数。

第二部分：对分离的字符识别的卷积网络 
从两个方面说了全连接网络的弊端，引出卷积网络的位移不变性(shift invariance)可以通过权值共享实现、提取特征通过将隐藏结点的感受野(local receptive fields)限制在了局部。(Convolutional Networks force the extraction of local features by restricting the receptive fields of hidden units to be local.)
A.卷积网络：CNN通过局部感受野(local receptive fields)，权值共享(shared weights)，下采样(sub-sampling)实现位移，缩放，和形变不变性(shift, scale, distortion invariance);LeNet-5，输入层是归一化并且字符位于中间的字符图像；每层的每个神经元的输入是上一层的局部几个神经元（局部感受野）；作卷积时同层每个不同的局部都共用同一个卷积核（权值共享）；在特征图中降低特征位置的精度的方式是降低特征图的空间分辨率，这个可以通过下采样层达到，下采样层通过求局部平均降低特征图的分辨率，并且降低了输出对平移和形变的敏感度；
B.LeNet-5：

输入图像大小：32\*32
1.C1层：6个5\*5的卷积核，步长1，则(32-5)+1=28，故输出28\*28\*6。神经元数量为28\*28\*6=784\*6=4704个。参数有((5\*5)+1)\*6=156个（1是bias偏置）。连接数一共156\*28\*28=122304个（C1每个神经元接受输入5\*5的局部图像并带1个偏置，所以1个神经元有26个连接）。
2.S2层：方式：2\*2输入相加乘以一个参数，加上一个偏置，通过sigmoid得到结果。6个2\*2池化，步长2，则((28-2)/2)+1=14，故输出14\*14\*6。神经元数量为14\*14\*6=1176个。参数有2\*6=12个（每层2个参数）。连接数一共5\*14\*14\*6=5880个（每个神经元接收4+1=5个输入）.

[./Receptive Field.png]

| NO. | Layer | Input Size | Kernel Size | Stride | Output Size | Receptive Field |       神经元数量      |     参数数量      |       连接数数量      |
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
|1    |C1     |32\*32      |      5\*5\*6|   1    |28\*28\*6    |        5       |           4704         |        156        |         122304        |
|2    |S2     |28\*28\*6   |      2\*2   |   2    |14\*14\*6    |        6       |           1176         |         12        |          5880         |




### 概要

LeNet正式提出cnn的思想，则AlexNet奠定了cnn的地位。
将图像分类从传统的分类方式正式带向cnn方向，由于数据和GPU的计算力共同提高下，构建了当时最大的卷积神经网络，并获得了最优结果。
同时论文突出了两方面的问题：
1.怎样在这个大型网络下提高性能的同时减少训练时间。
2.大型网络有大量的参数，怎样去防止过拟合。


### 怎样在这个大型网络下提高性能的同时减少训练时间

+ 提出ReLU激活函数
在CIFAR-10数据集上达到25%训练错误率比同等条件下使用tanh神经元快6倍
+ 多GPU训练
使用多GPU训练一方面是受单个GPU的显存限制，一方面是加快训练
在多GPU之间并不是所有层都互相通信，只在某些层进行交互通信。
+ 局部响应归一化
在论文中局部归一化确实使性能有所提高
（响应归一化将top-1和top-5的错误率分别降低了1.4%和1.2%）
但是在后续研究中，对于越大的网络。局部响应归一化并没有带来提升，所以这个技术被淡化，基本不再使用。
+ 重叠池化
池化步长小于池化尺寸就是重叠池化
实验证明重叠池化带来了性能上的提升（将top-1和top-5的错误率分别降低了0.4%和0.3%）
重叠池化也在一定程度上抑制了过拟合

【我的思考】
1.relu激活函数的特性让求导变得更加简单
2.多GPU让所有层都进行交互会产生怎样的区别？

###大型网络有大量的参数，怎样去防止过拟合

+ 数据增强
降低过拟合最简单直接的方式就是扩大数据集
论文中扩大数据集的方式：
1.从256x256的图像中随机提取224x224大小的块以及他们的翻转图像构成新的数据集（可扩展(256-224)2*2=2048倍）
预测方针是：提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。
2.改变训练图像的RGB通道的强度
相当于改变了图像的光照、颜色、强度，但是目标特性任然不变。

+ dropout
每一个隐藏神经元都有0.5的概率不参与前向传播和反向传播。
每一次迭代隐藏神经元的训练对象都是在变的

【我的思考】
1.越大的网络对数据的依赖越大，训练复杂网络必须要有足够的数据量支撑，在相似分类的情况下可以使用迁移学习从一定程度上减小这种依赖
2.drop让更多的隐藏神经元参与进来，让激活较少的神经元得到迭代更新，使得网络泛华能力更强



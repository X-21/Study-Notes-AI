# GoogLeNet
翻译：https://www.jianshu.com/p/f60ee8aa8d19 
资料：https://blog.csdn.net/shuzfan/article/details/50738394 
https://blog.csdn.net/stesha_chen/article/details/81662405 


### 概要

使用inception神经网络架构，在扩展网络深度的同时扩展其宽度（以往都是在深度上做文章）
宗旨在于最大的收获不是来自于越来越大的深度网络的简单应用，而是来自于深度架构和经典计算机视觉的协同

### 引入问题
论文中提出提高深度神经网络性能最直接的方式是增加它们的尺寸（深度－网络层数，宽度－每一层单元数目）
但是复杂的网络又带来了两个问题：
1.需要更多强标注的样本来训练
2.需要更强大的运算力
解决这两个问题的一个基本的方式就是引入稀疏性并将全连接层替换为稀疏的全连接层，甚至是卷积层。

### 架构
找到最优的局部构造（inception结构）并在空间上重复
在底层保持传统的卷积架构，只在高层做inception架构的堆叠（论文描述其为有益的，没有具体阐明缘由）

### 亮点
1.1x1卷积层的精妙使用
由于宽度卷积的堆叠使得参数计算非常大，导致卷积瓶颈，于是用1x1的卷积来降维，解除瓶颈对网络大小的限制
保证在网络深度的增加，同时宽度的增加而不失性能
将1x1的卷积层用在3x3,5x5卷积之前以及pooling之后
（有别于vgg中1x1的卷积主要用于增加非线性变换）

### 训练方式

+ 特殊的反向传播
网络的深度和宽度的增加导致结构非常复杂，而反向传播通过所有层的能力有限（梯度爆炸，梯度消失）
于是通过添加中间底层的辅助分类器（辅助分类的损失将以0.3的权重添加到整个模型的损失上），但是在最终判断时，这些辅助分类器是被丢失的
实验证明这样的辅助分类器数量上的增加带来的效果较小，只需要一个即可带来相同的效果。
其作用被认为是提供正则化的同时克服了梯度消失的问题

### 实验

+ 数据增强的饱和
图像裁剪不是越多越好，当达到一个合理的数量之后，图像裁剪的好处会变得越来越微小，所有要掌握好这个平衡点以免不必要的计算浪费
论文中用256，288，320和352四个尺寸做归一化（可以作为基本处理方法）

【我的思考】
1.GoogLeNet的提出给了神经网络以宽度，从另一个方面使得网络的性能更强
2.在结构上，大而简，寻找最优局部结构做堆叠
3.反向传播上使用中间输出控制全局输出，感觉很有新意，很牛掰


### V2&V3

General Design Principles
下面的准则来源于大量的实验，因此包含一定的推测，但实际证明基本都是有效的。 

1. 避免表达瓶颈，特别是在网络靠前的地方。 信息流前向传播过程中显然不能经过高度压缩的层，即表达瓶颈。从input到output，feature map的宽和高基本都会逐渐变小，但是不能一下子就变得很小。比如你上来就来个kernel = 7, stride = 5 ,这样显然不合适。另外输出的维度channel，一般来说会逐渐增多(每层的num_output)，否则网络会很难训练。（特征维度并不代表信息的多少，只是作为一种估计的手段） 

2. 高维特征更易处理。 高维特征更易区分，会加快训练。 

3. 可以在低维嵌入上进行空间汇聚而无需担心丢失很多信息。 比如在进行3x3卷积之前，可以对输入先进行降维而不会产生严重的后果。假设信息可以被简单压缩，那么训练就会加快。 

4. 平衡网络的宽度与深度。 

上述的这些并不能直接用来提高网络质量，而仅用来在大环境下作指导。 

提出了BN，并使用两个3x3的卷积计算替代V1中Inception中单个5x5的卷积计算。其中，BN有两个好处，一是对每一层的输入都做了“标准化”处理，预防梯度爆炸、消失，可以加快训练速度；二是减少了上一层参数的变化对下一层输入值的影响，有轻微正则化的作用。

# GoogLeNet
#翻译：https://www.jianshu.com/p/f60ee8aa8d19

### 概要

使用inception神经网络架构，在扩展网络深度的同时扩展其宽度（以往都是在深度上做文章）
宗旨在于最大的收获不是来自于越来越大的深度网络的简单应用，而是来自于深度架构和经典计算机视觉的协同

### 引入问题
论文中提出提高深度神经网络性能最直接的方式是增加它们的尺寸（深度－网络层数，宽度－每一层单元数目）
但是复杂的网络又带来了两个问题：
1.需要更多强标注的样本来训练
2.需要更强大的运算力
解决这两个问题的一个基本的方式就是引入稀疏性并将全连接层替换为稀疏的全连接层，甚至是卷积层。

### 架构
找到最优的局部构造（inception结构）并在空间上重复
在底层保持传统的卷积架构，只在高层做inception架构的堆叠（论文描述其为有益的，没有具体阐明缘由）

### 亮点
1.1x1卷积层的精妙使用
由于宽度卷积的堆叠使得参数计算非常大，导致卷积瓶颈，于是用1x1的卷积来降维，解除瓶颈对网络大小的限制
保证在网络深度的增加，同时宽度的增加而不失性能
将1x1的卷积层用在3x3,5x5卷积之前以及pooling之后
（有别于vgg中1x1的卷积主要用于增加非线性变换）

### 训练方式

+ 特殊的方向传播
网络的深度和宽度的增加导致结构非常复杂，而反向传播通过所有层的能力有限（梯度爆炸，梯度消失）
于是通过添加中间底层的辅助分类器（辅助分类的损失将以0.3的权重添加到整个模型的损失上），但是在最终判断时，这些辅助分类器是被丢失的
实验证明这样的辅助分类器数量上的增加带来的效果较小，只需要一个即可带来相同的效果。
其作用被认为是提供正则化的同时克服了梯度消失的问题

### 实验

+ 数据增强的饱和
图像裁剪不是越多越好，当达到一个合理的数量之后，图像裁剪的好处会变得越来越微小，所有要掌握好这个平衡点以免不必要的计算浪费
论文中用256，288，320和352四个尺寸做归一化（可以作为基本处理方法）

【我的思考】
1.GoogLeNet的提出给了神经网络以宽度，从另一个方面使得网络的性能更强
2.在结构上，大而简，寻找最优局部结构做堆叠
3.反向传播上使用中间输出控制全局输出，感觉很有新意，很牛掰




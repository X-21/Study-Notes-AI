# ResNet
翻译：https://blog.csdn.net/Quincuntial/article/details/77263562?locationNum=6 
资料：https://blog.csdn.net/mao_feng/article/details/52734438 
https://blog.csdn.net/qq_38807688/article/details/84259547 

### 概要

研究网络的深度，相比vgg，将网络深度发挥到极致。
深度网络自然地将低/中/高级特征和分类器以端到端多层方式进行集成，特征的“级别”可以通过堆叠层的数量（深度）来丰富。最近的证据显示网络深度至关重要
而网络的深度加深后优化存在很大的问题，一方面是参数的增加和退化问题。
（退化问题：网络加深复杂后，其错误率达到饱和后反而增加（不是过拟合引起的））
为解决上述问题提出了恒等映射的残差网络。
类似GoogleNet堆叠的方式，将残差块堆叠起来，在块与块之间建立恒等映射，从而达到深层网络。

### 残差学习

残差网络相比其他的网络在卷积层后的响应更小

我的思考：
更多的层堆叠，每一个层只要做好自己的那一些工作，综合起来就能发挥很大的作用
同时较小的响应输出将作为下一层的输入，这样的效果似乎类似于输出归一化，可以提高泛化能力

【摘抄】
网络输入是x，网络的输出是F(x)，网络要拟合的目标是H(x)，传统网络的训练目标是F(x)=H(x)。残差网络，则是把传统网络的输出F(x)处理一下，加上输入，将F(x)+x作为最终的输出，训练目标是F(x)=H(x)-x，因此得名残差网络。现在我们要训练一个深层的网络，它可能过深，假设存在一个性能最强的完美网络N，与它相比我们的网络中必定有一些层是多余的，那么这些多余的层的训练目标是恒等变换，只有达到这个目标我们的网络性能才能跟N一样。对于这些需要实现恒等变换的多余的层，要拟合的目标就成了H(x)=x，在传统网络中，网络的输出目标是F(x)=x，这比较困难，而在残差网络中，拟合的目标成了x-x=0，网络的输出目标为F(x)=0，这比前者要容易得多。 

作者：王殊 
链接：https://www.zhihu.com/question/53224378/answer/343061012 
来源：知乎 
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 

### 快捷恒等映射

通俗的讲输入可以直接加到这一层的输出上
但是如果在这一层的处理后，输出的维度和输入的维度有差异，则需要投影映射
投影映射有两种方式：
1.通过额外添加零输入增加维度（没有增加计算）
2.通过1x1的卷积层增加维度（1x1的卷积层在Vgg GoogLeNet中都得到的很好的使用）

### 网络结构

+ 瓶颈结构
为了达到更加深层的网络，3x3的网络卷积任然会带来很大的计算量
于是再次用1x1的卷积做了很牛掰的操作
1.使用1x1卷积先将输入维度减小后再去做3x3的卷积
2.然后再用1x1的卷积恢复其维度

### 实验

1.相比vgg等其他网络，残差网络成功减小了误差
2.恒等和投影快捷连接
三种链接方式：(A) 零填充快捷连接用来增加维度，所有的快捷连接是没有参数的；(B)投影快捷连接用来增加维度，其它的快捷连接是恒等的；（C）所有的快捷连接都是投影
其性能是逐步增加的，但是(C)相较于(B)没有带来很大的提升但是计算量增加了很多，不适于向更高层扩展，所以使用(B)的连接方式
3.随着残差网络层数的增加，性能是进一步增加的
但是在CIFAR-10的实验中，增加到1202的网络性能不如110层的网络
分析是因为没有足够的数据支撑而不是因为退化的原因	


【我的思考】
1.将cnn的深度研究到极致，通过快捷连接让高层可以继承浅层表示，不至于性能下降
2.这种网络似乎让模型有了自己对层数的选择，可以在多余的层数通过快捷连接到输出，只去激活自己需要的层数

【思考】
上面的思考2的意思应该是说，因为不知道深度网络中哪些层是冗余的，所以让网络自己选择，那些冗余的层会因为恒等映射“隐藏起来”。

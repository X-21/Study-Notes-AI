# ResNet
#翻译：https://blog.csdn.net/Quincuntial/article/details/77263562?locationNum=6

### 概要

研究网络的深度，相比vgg，将网络深度发挥到极致。
深度网络自然地将低/中/高级特征和分类器以端到端多层方式进行集成，特征的“级别”可以通过堆叠层的数量（深度）来丰富。最近的证据显示网络深度至关重要
而网络的深度加深后优化存在很大的问题，一方面是参数的增加和退化问题。
（退化问题：网络加深复杂后，其错误率达到饱和后反而增加（不是过拟合引起的））
为解决上述问题提出了恒等映射的残差网络。
类似GoogleNet堆叠的方式，将残差块堆叠起来，在块与块之间建立恒等映射，从而达到深层网络。

### 残差学习

残差网络相比其他的网络在卷积层后的响应更小

我的思考：
更多的层堆叠，每一个层只要做好自己的那一些工作，综合起来就能发挥很大的作用
同时较小的响应输出将作为下一层的输入，这样的效果似乎类似于输出归一化，可以提高泛化能力

### 快捷恒等映射

通俗的讲输入可以直接加到这一层的输出上
但是如果在这一层的处理后，输出的维度和输入的维度有差异，则需要投影映射
投影映射有两种方式：
1.通过额外添加零输入增加维度（没有增加计算）
2.通过1x1的卷积层增加维度（1x1的卷积层在Vgg GoogLeNet中都得到的很好的使用）

### 网络结构

+ 瓶颈结构
为了达到更加深层的网络，3x3的网络卷积任然会带来很大的计算量
于是再次用1x1的卷积做了很牛掰的操作
1.使用1x1卷积先将输入维度减小后再去做3x3的卷积
2.然后再用1x1的卷积恢复其维度

### 实验

1.相比vgg等其他网络，残差网络成功减小了误差
2.恒等和投影快捷连接
三种链接方式：(A) 零填充快捷连接用来增加维度，所有的快捷连接是没有参数的；(B)投影快捷连接用来增加维度，其它的快捷连接是恒等的；（C）所有的快捷连接都是投影
其性能是逐步增加的，但是(C)相较于(B)没有带来很大的提升但是计算量增加了很多，不适于向更高层扩展，所以使用(B)的连接方式
3.随着残差网络层数的增加，性能是进一步增加的
但是在CIFAR-10的实验中，增加到1202的网络性能不如110层的网络
分析是因为没有足够的数据支撑而不是因为退化的原因	


【我的思考】
1.将cnn的深度研究到极致，通过快捷连接让高层可以继承浅层表示，不至于性能下降
2.这种网络似乎让模型有了自己对层数的选择，可以在多余的层数通过快捷连接到输出，只去激活自己需要的层数

